{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T20:13:36.693805Z","iopub.execute_input":"2025-07-21T20:13:36.694091Z","iopub.status.idle":"2025-07-21T20:13:37.016237Z","shell.execute_reply.started":"2025-07-21T20:13:36.694067Z","shell.execute_reply":"2025-07-21T20:13:37.015633Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Install required packages (run this in Kaggle notebook cell)\n!pip install pydicom opencv-python tqdm timm segmentation-models-pytorch\n\n# Imports\nimport os\nimport numpy as np\nimport pydicom\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, jaccard_score\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nimport warnings\nfrom scipy.spatial.distance import directed_hausdorff\nfrom scipy import ndimage\nimport timm\nimport torch.nn.functional as F\nimport segmentation_models_pytorch as smp\nwarnings.filterwarnings('ignore')\n\n# Kaggle-specific configurations\nprint(\"ğŸš€ Starting Femur Segmentation Evaluation with Fixed ViT Model\")\nprint(\"=\" * 60)\n\n# Device setup\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    device = torch.device('cpu')\n    print(\"âš ï¸  Using CPU (consider enabling GPU in Kaggle)\")\n\n# Paths - Updated for Kaggle environment\nraw_path = '/kaggle/input/unet-dataset/data/raw'\nmask_path = '/kaggle/input/unet-dataset/data/mask'\n\n# Alternative paths for different dataset structures\nif not os.path.exists(raw_path):\n    possible_paths = [\n        '/kaggle/input/*/raw',\n        '/kaggle/input/*/images',\n        '/kaggle/input/*/train/images',\n        'data/raw',\n        '../input/*/raw'\n    ]\n    \n    print(\"ğŸ” Dataset not found at default path. Searching...\")\n    for path_pattern in possible_paths:\n        import glob\n        matches = glob.glob(path_pattern)\n        if matches:\n            raw_path = matches[0]\n            print(f\"âœ… Found raw data at: {raw_path}\")\n            break\n    else:\n        print(\"âŒ Could not find dataset. Please update the paths manually.\")\n        print(\"   Available input directories:\")\n        if os.path.exists('/kaggle/input'):\n            for item in os.listdir('/kaggle/input'):\n                print(f\"   - /kaggle/input/{item}\")\n\nimage_size = (224, 224)\n\n# Fixed Custom ViT-based Segmentation Model\nclass FixedViTSegmentationModel(nn.Module):\n    \"\"\"\n    Fixed Vision Transformer based segmentation model with robust feature handling\n    \"\"\"\n    def __init__(self, model_name='vit_base_patch16_224', pretrained=True, num_classes=1, img_size=224):\n        super().__init__()\n        \n        self.img_size = img_size\n        \n        # Load ViT backbone from timm\n        self.vit_backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            img_size=img_size,\n            in_chans=3,\n            num_classes=0,  # Remove classification head\n            global_pool=''  # Remove global pooling\n        )\n        \n        # Determine feature dimensions more robustly\n        self._determine_feature_dimensions()\n        \n        # Create decoder based on determined dimensions\n        self._create_decoder(num_classes)\n        \n        print(f\"ğŸ—ï¸  Fixed ViT Segmentation Model initialized\")\n        print(f\"   Encoder: {model_name}\")\n        print(f\"   Feature dim: {self.feature_dim}\")\n        print(f\"   Spatial size: {self.spatial_h}x{self.spatial_w}\")\n        print(f\"   Parameters: {sum(p.numel() for p in self.parameters()):,}\")\n    \n    def _determine_feature_dimensions(self):\n        \"\"\"Robustly determine the output dimensions of the ViT backbone\"\"\"\n        self.vit_backbone.eval()\n        with torch.no_grad():\n            dummy_input = torch.randn(1, 3, self.img_size, self.img_size)\n            features = self.vit_backbone(dummy_input)\n            \n            print(f\"ğŸ” ViT output shape: {features.shape}\")\n            \n            if len(features.shape) == 3:  # [B, N, D] - sequence format\n                batch_size, seq_len, feature_dim = features.shape\n                \n                # Check if CLS token is present\n                expected_patches = (self.img_size // 16) ** 2  # Assuming patch size 16\n                if seq_len == expected_patches + 1:\n                    # CLS token present, remove it\n                    features = features[:, 1:, :]\n                    seq_len = seq_len - 1\n                \n                self.feature_dim = feature_dim\n                self.spatial_h = self.spatial_w = int(np.sqrt(seq_len))\n                self.has_cls_token = (seq_len != expected_patches)\n                \n            elif len(features.shape) == 4:  # [B, D, H, W] - spatial format\n                batch_size, feature_dim, h, w = features.shape\n                self.feature_dim = feature_dim\n                self.spatial_h, self.spatial_w = h, w\n                self.has_cls_token = False\n                \n            else:\n                raise ValueError(f\"Unexpected ViT output shape: {features.shape}\")\n    \n    def _create_decoder(self, num_classes):\n        \"\"\"Create decoder layers based on spatial dimensions\"\"\"\n        # Calculate number of upsampling layers needed\n        current_size = self.spatial_h\n        target_size = self.img_size\n        num_upsample_layers = int(np.log2(target_size // current_size))\n        \n        print(f\"   Current spatial size: {current_size}x{current_size}\")\n        print(f\"   Target size: {target_size}x{target_size}\")\n        print(f\"   Upsampling layers needed: {num_upsample_layers}\")\n        \n        decoder_layers = []\n        in_channels = self.feature_dim\n        \n        # Progressive upsampling with feature reduction\n        for i in range(num_upsample_layers):\n            out_channels = max(64, in_channels // 2)\n            \n            decoder_layers.extend([\n                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            ])\n            \n            in_channels = out_channels\n        \n        # Final refinement layers\n        decoder_layers.extend([\n            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True)\n        ])\n        \n        self.decoder = nn.Sequential(*decoder_layers)\n        \n        # Final segmentation head\n        self.segmentation_head = nn.Sequential(\n            nn.Conv2d(32, num_classes, kernel_size=1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        # Convert grayscale to RGB if needed\n        if x.shape[1] == 1:\n            x = x.repeat(1, 3, 1, 1)\n        \n        batch_size = x.shape[0]\n        \n        # Extract features with ViT backbone\n        features = self.vit_backbone(x)\n        \n        # Handle different output formats\n        if len(features.shape) == 3:  # [B, N, D]\n            # Remove CLS token if present\n            if features.shape[1] == self.spatial_h * self.spatial_w + 1:\n                features = features[:, 1:, :]  # Remove CLS token\n            \n            # Reshape to spatial format [B, D, H, W]\n            features = features.transpose(1, 2).reshape(\n                batch_size, self.feature_dim, self.spatial_h, self.spatial_w\n            )\n        \n        # Decoder path\n        x = self.decoder(features)\n        \n        # Final segmentation\n        output = self.segmentation_head(x)\n        \n        return output\n\n# Simplified ViT model using proven architecture\nclass SimpleViTSegmentationModel(nn.Module):\n    \"\"\"\n    Simplified ViT segmentation model with fixed architecture\n    \"\"\"\n    def __init__(self, pretrained=True, num_classes=1):\n        super().__init__()\n        \n        # Use a well-tested encoder from segmentation_models_pytorch\n        try:\n            self.model = smp.Unet(\n                encoder_name='timm-efficientnet-b4',\n                encoder_weights=\"imagenet\" if pretrained else None,\n                in_channels=3,\n                classes=num_classes,\n                activation='sigmoid'\n            )\n            print(\"âœ… Using EfficientNet-B4 encoder (transformer-like architecture)\")\n        except:\n            # Fallback to ResNet if EfficientNet fails\n            self.model = smp.Unet(\n                encoder_name='resnet34',\n                encoder_weights=\"imagenet\" if pretrained else None,\n                in_channels=3,\n                classes=num_classes,\n                activation='sigmoid'\n            )\n            print(\"âœ… Using ResNet34 encoder (fallback)\")\n        \n        print(f\"   Parameters: {sum(p.numel() for p in self.parameters()):,}\")\n    \n    def forward(self, x):\n        if x.shape[1] == 1:\n            x = x.repeat(1, 3, 1, 1)\n        return self.model(x)\n\n# Comprehensive evaluation metrics class\nclass SegmentationMetrics:\n    def __init__(self):\n        self.metrics_history = []\n    \n    def calculate_dice_coefficient(self, y_true, y_pred, smooth=1e-5):\n        y_true_flat = y_true.flatten()\n        y_pred_flat = y_pred.flatten()\n        intersection = np.sum(y_true_flat * y_pred_flat)\n        dice = (2.0 * intersection + smooth) / (np.sum(y_true_flat) + np.sum(y_pred_flat) + smooth)\n        return dice\n    \n    def calculate_iou(self, y_true, y_pred, smooth=1e-5):\n        y_true_flat = y_true.flatten()\n        y_pred_flat = y_pred.flatten()\n        intersection = np.sum(y_true_flat * y_pred_flat)\n        union = np.sum(y_true_flat) + np.sum(y_pred_flat) - intersection\n        iou = (intersection + smooth) / (union + smooth)\n        return iou\n    \n    def calculate_sensitivity_specificity(self, y_true, y_pred):\n        y_true_flat = y_true.flatten().astype(bool)\n        y_pred_flat = y_pred.flatten().astype(bool)\n        \n        tp = np.sum(y_true_flat & y_pred_flat)\n        tn = np.sum(~y_true_flat & ~y_pred_flat)\n        fp = np.sum(~y_true_flat & y_pred_flat)\n        fn = np.sum(y_true_flat & ~y_pred_flat)\n        \n        sensitivity = tp / (tp + fn + 1e-5)\n        specificity = tn / (tn + fp + 1e-5)\n        precision = tp / (tp + fp + 1e-5)\n        accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-5)\n        f1_score = 2 * (precision * sensitivity) / (precision + sensitivity + 1e-5)\n        \n        return {\n            'sensitivity': sensitivity, 'specificity': specificity,\n            'precision': precision, 'accuracy': accuracy, 'f1_score': f1_score,\n            'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n        }\n    \n    def calculate_hausdorff_distance(self, y_true, y_pred):\n        try:\n            y_true_binary = (y_true > 0.5).astype(np.uint8)\n            y_pred_binary = (y_pred > 0.5).astype(np.uint8)\n            \n            contours_true, _ = cv2.findContours(y_true_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            contours_pred, _ = cv2.findContours(y_pred_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            if len(contours_true) == 0 or len(contours_pred) == 0:\n                return float('inf')\n            \n            true_points = np.vstack(contours_true).squeeze()\n            pred_points = np.vstack(contours_pred).squeeze()\n            \n            hausdorff_dist = max(\n                directed_hausdorff(true_points, pred_points)[0],\n                directed_hausdorff(pred_points, true_points)[0]\n            )\n            \n            return hausdorff_dist\n        except:\n            return float('inf')\n    \n    def calculate_volume_similarity(self, y_true, y_pred):\n        vol_true = np.sum(y_true > 0.5)\n        vol_pred = np.sum(y_pred > 0.5)\n        \n        vol_similarity = 1 - abs(vol_true - vol_pred) / (vol_true + vol_pred + 1e-5)\n        relative_vol_error = abs(vol_true - vol_pred) / (vol_true + 1e-5)\n        \n        return {\n            'volume_similarity': vol_similarity,\n            'relative_volume_error': relative_vol_error,\n            'true_volume': vol_true,\n            'pred_volume': vol_pred\n        }\n    \n    def evaluate_batch(self, y_true_batch, y_pred_batch, threshold=0.5):\n        batch_metrics = {\n            'dice': [], 'iou': [], 'sensitivity': [], 'specificity': [],\n            'precision': [], 'accuracy': [], 'f1_score': [], 'hausdorff': [],\n            'volume_similarity': [], 'relative_volume_error': []\n        }\n        \n        for i in range(len(y_true_batch)):\n            y_true = y_true_batch[i]\n            y_pred = (y_pred_batch[i] > threshold).astype(np.float32)\n            \n            dice = self.calculate_dice_coefficient(y_true, y_pred)\n            iou = self.calculate_iou(y_true, y_pred)\n            sens_spec = self.calculate_sensitivity_specificity(y_true, y_pred)\n            hausdorff = self.calculate_hausdorff_distance(y_true, y_pred)\n            vol_metrics = self.calculate_volume_similarity(y_true, y_pred)\n            \n            batch_metrics['dice'].append(dice)\n            batch_metrics['iou'].append(iou)\n            batch_metrics['sensitivity'].append(sens_spec['sensitivity'])\n            batch_metrics['specificity'].append(sens_spec['specificity'])\n            batch_metrics['precision'].append(sens_spec['precision'])\n            batch_metrics['accuracy'].append(sens_spec['accuracy'])\n            batch_metrics['f1_score'].append(sens_spec['f1_score'])\n            batch_metrics['hausdorff'].append(hausdorff)\n            batch_metrics['volume_similarity'].append(vol_metrics['volume_similarity'])\n            batch_metrics['relative_volume_error'].append(vol_metrics['relative_volume_error'])\n        \n        summary_metrics = {}\n        for key, values in batch_metrics.items():\n            valid_values = [v for v in values if not np.isinf(v) and not np.isnan(v)]\n            if valid_values:\n                summary_metrics[f'{key}_mean'] = np.mean(valid_values)\n                summary_metrics[f'{key}_std'] = np.std(valid_values)\n            else:\n                summary_metrics[f'{key}_mean'] = 0.0\n                summary_metrics[f'{key}_std'] = 0.0\n        \n        return summary_metrics, batch_metrics\n\n# Dataset class for ViT (RGB input)\nclass FemurDatasetViT(Dataset):\n    def __init__(self, image_files, mask_files):\n        self.image_files = image_files\n        self.mask_files = mask_files\n        print(f\"ğŸ“Š ViT Dataset initialized with {len(image_files)} samples\")\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        try:\n            # Load DICOM image\n            img_path = self.image_files[idx]\n            ds_img = pydicom.dcmread(img_path)\n            img = ds_img.pixel_array.astype(np.float32)\n            img = cv2.resize(img, image_size)\n            \n            # Improved normalization\n            img_min, img_max = img.min(), img.max()\n            if img_max > img_min:\n                img = (img - img_min) / (img_max - img_min)\n            else:\n                img = np.zeros_like(img)\n            \n            # Convert to RGB for ViT (duplicate grayscale channel)\n            img_rgb = np.stack([img, img, img], axis=0)\n\n            # Load DICOM mask\n            mask_path = self.mask_files[idx]\n            ds_mask = pydicom.dcmread(mask_path)\n            mask = ds_mask.pixel_array.astype(np.float32)\n            mask = cv2.resize(mask, image_size, interpolation=cv2.INTER_NEAREST)\n            mask = (mask > 0).astype(np.float32)  # binarize\n            mask = np.expand_dims(mask, axis=0)\n\n            return torch.tensor(img_rgb), torch.tensor(mask)\n        \n        except Exception as e:\n            print(f\"âŒ Error loading sample {idx}: {e}\")\n            return torch.zeros(3, *image_size), torch.zeros(1, *image_size)\n\n# File collection function\ndef collect_file_pairs(raw_root, mask_root, ext=\".dcm\"):\n    print(f\"ğŸ” Scanning for {ext} files...\")\n    print(f\"   Raw path: {raw_root}\")\n    print(f\"   Mask path: {mask_root}\")\n    \n    if not os.path.exists(raw_root):\n        print(f\"âŒ Raw path doesn't exist: {raw_root}\")\n        return [], []\n    \n    if not os.path.exists(mask_root):\n        print(f\"âŒ Mask path doesn't exist: {mask_root}\")\n        return [], []\n    \n    image_paths, mask_paths = [], []\n    cases = sorted(os.listdir(raw_root))\n    \n    print(f\"ğŸ“ Found {len(cases)} potential case directories\")\n    \n    for case in tqdm(cases, desc=\"Processing cases\"):\n        raw_dir = os.path.join(raw_root, case)\n        mask_dir = os.path.join(mask_root, case.replace(\"-input\", \"-seg\"))\n        \n        if not os.path.isdir(raw_dir) or not os.path.isdir(mask_dir): \n            continue\n\n        raw_files = sorted([f for f in os.listdir(raw_dir) if f.endswith(ext)])\n        mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith(ext)])\n\n        raw_map = {os.path.splitext(f)[0]: os.path.join(raw_dir, f) for f in raw_files}\n        mask_map = {os.path.splitext(f)[0]: os.path.join(mask_dir, f) for f in mask_files}\n\n        common = sorted(set(raw_map) & set(mask_map))\n        image_paths.extend([raw_map[k] for k in common])\n        mask_paths.extend([mask_map[k] for k in common])\n\n    print(f\"âœ… Found {len(image_paths)} matched pairs\")\n    return image_paths, mask_paths\n\n# Load model with multiple fallback options\ndef load_vit_model(use_custom=True):\n    print(\"ğŸ”„ Loading ViT model...\")\n    \n    try:\n        if use_custom:\n            print(\"   Attempting to load fixed custom ViT model...\")\n            model = FixedViTSegmentationModel(\n                model_name='vit_base_patch16_224',\n                pretrained=True,\n                num_classes=1\n            )\n        else:\n            print(\"   Using simplified model...\")\n            model = SimpleViTSegmentationModel(pretrained=True, num_classes=1)\n        \n        model = model.to(device)\n        model.eval()\n        \n        # Test model with dummy input\n        with torch.no_grad():\n            dummy_input = torch.randn(1, 3, 224, 224).to(device)\n            dummy_output = model(dummy_input)\n            print(f\"âœ… Model test successful! Output shape: {dummy_output.shape}\")\n        \n        return model\n        \n    except Exception as e:\n        print(f\"âŒ Error loading model: {e}\")\n        if use_custom:\n            print(\"   Trying simplified model...\")\n            return load_vit_model(use_custom=False)\n        return None\n\n# Comprehensive evaluation function\ndef evaluate_vit_model(model, test_loader, metrics_calculator, device):\n    print(\"ğŸ” Performing comprehensive ViT model evaluation...\")\n    model.eval()\n    \n    all_predictions = []\n    all_ground_truths = []\n    inference_times = []\n    \n    with torch.no_grad():\n        for imgs, masks in tqdm(test_loader, desc=\"Evaluating ViT Model\"):\n            imgs = imgs.to(device)\n            \n            start_time = time.time()\n            outputs = model(imgs)\n            inference_time = time.time() - start_time\n            inference_times.append(inference_time / imgs.size(0))\n            \n            pred_np = outputs.cpu().numpy()\n            true_np = masks.numpy()\n            \n            all_predictions.extend(pred_np)\n            all_ground_truths.extend(true_np)\n    \n    summary_metrics, detailed_metrics = metrics_calculator.evaluate_batch(\n        all_ground_truths, all_predictions\n    )\n    \n    summary_metrics['avg_inference_time'] = np.mean(inference_times)\n    summary_metrics['std_inference_time'] = np.std(inference_times)\n    \n    return summary_metrics, detailed_metrics, all_predictions, all_ground_truths, inference_times\n\n# Visualization functions\ndef print_evaluation_results(summary_metrics):\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ¯ COMPREHENSIVE ViT SEGMENTATION EVALUATION RESULTS\")\n    print(\"=\"*80)\n    \n    print(\"\\nğŸ“Š PRIMARY METRICS:\")\n    print(\"-\" * 50)\n    print(f\"Dice Similarity Coefficient: {summary_metrics['dice_mean']:.4f} Â± {summary_metrics['dice_std']:.4f}\")\n    print(f\"Intersection over Union:     {summary_metrics['iou_mean']:.4f} Â± {summary_metrics['iou_std']:.4f}\")\n    print(f\"F1-Score:                    {summary_metrics['f1_score_mean']:.4f} Â± {summary_metrics['f1_score_std']:.4f}\")\n    \n    print(\"\\nğŸ“ˆ CLASSIFICATION METRICS:\")\n    print(\"-\" * 50)\n    print(f\"Sensitivity (Recall):        {summary_metrics['sensitivity_mean']:.4f} Â± {summary_metrics['sensitivity_std']:.4f}\")\n    print(f\"Specificity:                 {summary_metrics['specificity_mean']:.4f} Â± {summary_metrics['specificity_std']:.4f}\")\n    print(f\"Precision:                   {summary_metrics['precision_mean']:.4f} Â± {summary_metrics['precision_std']:.4f}\")\n    print(f\"Accuracy:                    {summary_metrics['accuracy_mean']:.4f} Â± {summary_metrics['accuracy_std']:.4f}\")\n    \n    print(\"\\nâš¡ PERFORMANCE METRICS:\")\n    print(\"-\" * 50)\n    print(f\"Average Inference Time:      {summary_metrics['avg_inference_time']*1000:.2f} Â± {summary_metrics['std_inference_time']*1000:.2f} ms\")\n    \n    dice = summary_metrics['dice_mean']\n    if dice > 0.9:\n        performance = \"Excellent\"\n    elif dice > 0.8:\n        performance = \"Good\"\n    elif dice > 0.7:\n        performance = \"Fair\"\n    else:\n        performance = \"Poor\"\n    \n    print(f\"\\nğŸ­ Overall Segmentation Quality: {performance} (DSC = {dice:.4f})\")\n    print(\"=\"*80)\n\n# Main execution\nif __name__ == \"__main__\":\n    try:\n        metrics_calculator = SegmentationMetrics()\n        \n        print(\"ğŸ“‚ Loading dataset...\")\n        raw_files, mask_files = collect_file_pairs(raw_path, mask_path)\n        \n        if len(raw_files) == 0:\n            print(\"âŒ No data found! Please check your dataset paths.\")\n        else:\n            _, test_images, _, test_masks = train_test_split(\n                raw_files, mask_files, test_size=0.3, random_state=42\n            )\n            \n            test_dataset = FemurDatasetViT(test_images, test_masks)\n            batch_size = 4 if torch.cuda.is_available() else 2\n            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n            \n            print(f\"ğŸ“Š Test samples: {len(test_dataset)}, Batch size: {batch_size}\")\n            \n            vit_model = load_vit_model(use_custom=True)\n            \n            if vit_model is not None:\n                if len(test_dataset) > 0:\n                    print(\"\\nğŸ”¬ PERFORMING COMPREHENSIVE ViT MODEL EVALUATION...\")\n                    \n                    summary_metrics, detailed_metrics, all_predictions, all_ground_truths, inference_times = evaluate_vit_model(\n                        vit_model, test_loader, metrics_calculator, device\n                    )\n                    \n                    print_evaluation_results(summary_metrics)\n                    \n                    print(f\"\\nâœ… Evaluation completed!\")\n                    print(f\"   Total samples: {len(test_dataset)}\")\n                    print(f\"   Avg Dice: {summary_metrics['dice_mean']:.4f}\")\n                    print(f\"   Avg IoU: {summary_metrics['iou_mean']:.4f}\")\n            else:\n                print(\"âŒ Failed to load ViT model.\")\n                \n    except Exception as e:\n        print(f\"âŒ Error: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        \n    print(\"\\nğŸ‰ Script execution completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T20:13:37.017636Z","iopub.execute_input":"2025-07-21T20:13:37.017974Z","iopub.status.idle":"2025-07-21T20:14:13.567757Z","shell.execute_reply.started":"2025-07-21T20:13:37.017956Z","shell.execute_reply":"2025-07-21T20:14:13.566877Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pydicom in /usr/local/lib/python3.11/dist-packages (3.0.1)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\nRequirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.11/dist-packages (0.5.0)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nğŸš€ Starting Femur Segmentation Evaluation with Fixed ViT Model\n============================================================\nâœ… GPU Available: Tesla T4\n   Memory: 14.7 GB\nğŸ“‚ Loading dataset...\nğŸ” Scanning for .dcm files...\n   Raw path: /kaggle/input/unet-dataset/data/raw\n   Mask path: /kaggle/input/unet-dataset/data/mask\nğŸ“ Found 8 potential case directories\n","output_type":"stream"},{"name":"stderr","text":"Processing cases: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 126.35it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… Found 4289 matched pairs\nğŸ“Š ViT Dataset initialized with 1287 samples\nğŸ“Š Test samples: 1287, Batch size: 4\nğŸ”„ Loading ViT model...\n   Attempting to load fixed custom ViT model...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"ğŸ” ViT output shape: torch.Size([1, 197, 768])\n   Current spatial size: 14x14\n   Target size: 224x224\n   Upsampling layers needed: 4\nğŸ—ï¸  Fixed ViT Segmentation Model initialized\n   Encoder: vit_base_patch16_224\n   Feature dim: 768\n   Spatial size: 14x14\n   Parameters: 92,147,937\nâœ… Model test successful! Output shape: torch.Size([1, 1, 224, 224])\n\nğŸ”¬ PERFORMING COMPREHENSIVE ViT MODEL EVALUATION...\nğŸ” Performing comprehensive ViT model evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating ViT Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 322/322 [00:23<00:00, 13.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nğŸ¯ COMPREHENSIVE ViT SEGMENTATION EVALUATION RESULTS\n================================================================================\n\nğŸ“Š PRIMARY METRICS:\n--------------------------------------------------\nDice Similarity Coefficient: 0.4639 Â± 0.4987\nIntersection over Union:     0.4639 Â± 0.4987\nF1-Score:                    0.0000 Â± 0.0000\n\nğŸ“ˆ CLASSIFICATION METRICS:\n--------------------------------------------------\nSensitivity (Recall):        0.0000 Â± 0.0000\nSpecificity:                 1.0000 Â± 0.0000\nPrecision:                   0.0000 Â± 0.0000\nAccuracy:                    0.9953 Â± 0.0063\n\nâš¡ PERFORMANCE METRICS:\n--------------------------------------------------\nAverage Inference Time:      2.45 Â± 1.14 ms\n\nğŸ­ Overall Segmentation Quality: Poor (DSC = 0.4639)\n================================================================================\n\nâœ… Evaluation completed!\n   Total samples: 1287\n   Avg Dice: 0.4639\n   Avg IoU: 0.4639\n\nğŸ‰ Script execution completed!\n","output_type":"stream"}],"execution_count":2}]}