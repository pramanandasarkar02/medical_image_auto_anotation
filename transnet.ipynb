{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfbb1653",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-21T20:20:09.672592Z",
     "iopub.status.busy": "2025-07-21T20:20:09.672175Z",
     "iopub.status.idle": "2025-07-21T20:20:11.676059Z",
     "shell.execute_reply": "2025-07-21T20:20:11.674945Z"
    },
    "papermill": {
     "duration": 2.009702,
     "end_time": "2025-07-21T20:20:11.677998",
     "exception": false,
     "start_time": "2025-07-21T20:20:09.668296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a053eceb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T20:20:11.684803Z",
     "iopub.status.busy": "2025-07-21T20:20:11.684054Z",
     "iopub.status.idle": "2025-07-21T20:20:32.810886Z",
     "shell.execute_reply": "2025-07-21T20:20:32.809502Z"
    },
    "papermill": {
     "duration": 21.132326,
     "end_time": "2025-07-21T20:20:32.812632",
     "exception": false,
     "start_time": "2025-07-21T20:20:11.680306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Femur Segmentation Training with TransUNet (ViT-B/16)\n",
      "============================================================\n",
      "‚ö†Ô∏è Using CPU\n",
      "üìÇ Loading dataset...\n",
      "üîç Scanning for .dcm files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cases: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 30.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 4289 matched pairs\n",
      "üìä Dataset initialized with 3431 samples\n",
      "üìä Dataset initialized with 858 samples\n",
      "üìä Training samples: 3431\n",
      "üìä Validation samples: 858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 330M/330M [00:02<00:00, 169MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è TransUNet initialized with 100,109,825 parameters\n",
      "üöÄ Starting training for 5 epochs\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]:   0%|          | 0/1716 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error occurred: too many indices for tensor of dimension 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_13/318753125.py\", line 376, in <cell line: 0>\n",
      "    train_losses, val_losses, dice_scores = train_model(\n",
      "                                            ^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13/318753125.py\", line 243, in train_model\n",
      "    outputs = model(imgs)\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13/318753125.py\", line 187, in forward\n",
      "    vit_features = vit_features[:, 1:, :].view(b, 768, 14, 14)  # Exclude CLS token\n",
      "                   ~~~~~~~~~~~~^^^^^^^^^^\n",
      "IndexError: too many indices for tensor of dimension 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import torchvision.models as models\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ Starting Femur Segmentation Training with TransUNet (ViT-B/16)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Using CPU\")\n",
    "\n",
    "# Dataset paths\n",
    "raw_path = '/kaggle/input/unet-dataset/data/raw'\n",
    "mask_path = '/kaggle/input/unet-dataset/data/mask'\n",
    "image_size = (224, 224)\n",
    "\n",
    "class SegmentationMetrics:\n",
    "    @staticmethod\n",
    "    def dice_coefficient(y_true, y_pred, smooth=1e-5):\n",
    "        y_true_flat = y_true.flatten()\n",
    "        y_pred_flat = y_pred.flatten()\n",
    "        intersection = np.sum(y_true_flat * y_pred_flat)\n",
    "        return (2.0 * intersection + smooth) / (np.sum(y_true_flat) + np.sum(y_pred_flat) + smooth)\n",
    "    \n",
    "    @staticmethod\n",
    "    def iou_score(y_true, y_pred, smooth=1e-5):\n",
    "        y_true_flat = y_true.flatten()\n",
    "        y_pred_flat = y_pred.flatten()\n",
    "        intersection = np.sum(y_true_flat * y_pred_flat)\n",
    "        union = np.sum(y_true_flat) + np.sum(y_pred_flat) - intersection\n",
    "        return (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    @classmethod\n",
    "    def evaluate_batch(cls, y_true_batch, y_pred_batch, threshold=0.5):\n",
    "        dice_scores = []\n",
    "        iou_scores = []\n",
    "        \n",
    "        for i in range(len(y_true_batch)):\n",
    "            y_true = y_true_batch[i]\n",
    "            y_pred = (y_pred_batch[i] > threshold).astype(np.float32)\n",
    "            \n",
    "            dice = cls.dice_coefficient(y_true, y_pred)\n",
    "            iou = cls.iou_score(y_true, y_pred)\n",
    "            \n",
    "            dice_scores.append(dice)\n",
    "            iou_scores.append(iou)\n",
    "        \n",
    "        return {\n",
    "            'dice_mean': np.mean(dice_scores),\n",
    "            'dice_std': np.std(dice_scores),\n",
    "            'iou_mean': np.mean(iou_scores),\n",
    "            'iou_std': np.std(iou_scores),\n",
    "            'dice_scores': dice_scores,\n",
    "            'iou_scores': iou_scores\n",
    "        }\n",
    "\n",
    "class FemurDataset(Dataset):\n",
    "    def __init__(self, image_files, mask_files):\n",
    "        self.image_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        print(f\"üìä Dataset initialized with {len(image_files)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Load and process image\n",
    "            img_path = self.image_files[idx]\n",
    "            ds_img = pydicom.dcmread(img_path)\n",
    "            img = ds_img.pixel_array.astype(np.float32)\n",
    "            img = cv2.resize(img, image_size)\n",
    "            \n",
    "            # Normalize\n",
    "            img_min, img_max = img.min(), img.max()\n",
    "            if img_max > img_min:\n",
    "                img = (img - img_min) / (img_max - img_min)\n",
    "            else:\n",
    "                img = np.zeros_like(img)\n",
    "            \n",
    "            # Convert to 3 channels for ViT\n",
    "            img = np.stack([img] * 3, axis=0)\n",
    "            \n",
    "            # Load and process mask\n",
    "            mask_path = self.mask_files[idx]\n",
    "            ds_mask = pydicom.dcmread(mask_path)\n",
    "            mask = ds_mask.pixel_array.astype(np.float32)\n",
    "            mask = cv2.resize(mask, image_size, interpolation=cv2.INTER_NEAREST)\n",
    "            mask = (mask > 0).astype(np.float32)\n",
    "            mask = np.expand_dims(mask, axis=0)\n",
    "\n",
    "            return torch.tensor(img), torch.tensor(mask)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading sample {idx}: {e}\")\n",
    "            return torch.zeros(3, *image_size), torch.zeros(1, *image_size)\n",
    "\n",
    "def collect_file_pairs(raw_root, mask_root, ext=\".dcm\"):\n",
    "    print(f\"üîç Scanning for {ext} files...\")\n",
    "    \n",
    "    if not os.path.exists(raw_root) or not os.path.exists(mask_root):\n",
    "        print(f\"‚ùå Paths don't exist: {raw_root}, {mask_root}\")\n",
    "        return [], []\n",
    "    \n",
    "    image_paths, mask_paths = [], []\n",
    "    cases = sorted(os.listdir(raw_root))\n",
    "    \n",
    "    for case in tqdm(cases, desc=\"Processing cases\"):\n",
    "        raw_dir = os.path.join(raw_root, case)\n",
    "        mask_dir = os.path.join(mask_root, case.replace(\"-input\", \"-seg\"))\n",
    "        \n",
    "        if not os.path.isdir(raw_dir) or not os.path.isdir(mask_dir): \n",
    "            continue\n",
    "\n",
    "        raw_files = sorted([f for f in os.listdir(raw_dir) if f.endswith(ext)])\n",
    "        mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith(ext)])\n",
    "\n",
    "        raw_map = {os.path.splitext(f)[0]: os.path.join(raw_dir, f) for f in raw_files}\n",
    "        mask_map = {os.path.splitext(f)[0]: os.path.join(mask_dir, f) for f in mask_files}\n",
    "\n",
    "        common = set(raw_map) & set(mask_map)\n",
    "        image_paths.extend([raw_map[k] for k in sorted(common)])\n",
    "        mask_paths.extend([mask_map[k] for k in sorted(common)])\n",
    "\n",
    "    print(f\"‚úÖ Found {len(image_paths)} matched pairs\")\n",
    "    return image_paths, mask_paths\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, vit_pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained ViT-B/16\n",
    "        self.vit = models.vit_b_16(weights='IMAGENET1K_V1' if vit_pretrained else None)\n",
    "        self.vit.heads = nn.Identity()  # Remove classification head\n",
    "        \n",
    "        # Decoder blocks\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.up4 = nn.ConvTranspose2d(768, 512, 2, 2)\n",
    "        self.dec4 = conv_block(768 + 512, 512)\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        self.dec3 = conv_block(512 + 256, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        self.dec2 = conv_block(256 + 128, 128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        self.dec1 = conv_block(128 + 64, 64)\n",
    "        \n",
    "        # Final layer\n",
    "        self.final = nn.Conv2d(64, 1, 1)\n",
    "        \n",
    "        # Skip connection adapters\n",
    "        self.skip_conv1 = nn.Conv2d(3, 64, 1)\n",
    "        self.skip_conv2 = nn.Conv2d(768, 128, 1)\n",
    "        self.skip_conv3 = nn.Conv2d(768, 256, 1)\n",
    "        self.skip_conv4 = nn.Conv2d(768, 512, 1)\n",
    "        \n",
    "        print(f\"üèóÔ∏è TransUNet initialized with {sum(p.numel() for p in self.parameters()):,} parameters\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ViT encoder\n",
    "        vit_features = self.vit(x)  # [batch, num_patches, 768]\n",
    "        \n",
    "        # Reshape to spatial dimensions (14x14 for 224x224 input)\n",
    "        b = x.shape[0]\n",
    "        vit_features = vit_features[:, 1:, :].view(b, 768, 14, 14)  # Exclude CLS token\n",
    "        \n",
    "        # Generate skip connections\n",
    "        skip1 = self.skip_conv1(x)\n",
    "        skip2 = self.skip_conv2(vit_features)\n",
    "        skip3 = self.skip_conv3(vit_features)\n",
    "        skip4 = self.skip_conv4(vit_features)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d4 = self.dec4(torch.cat([self.up4(vit_features), skip4], 1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), skip3], 1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), skip2], 1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), skip1], 1))\n",
    "        \n",
    "        return torch.sigmoid(self.final(d1))\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, dice_weight=0.5, bce_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce_weight = bce_weight\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true, smooth=1e-5):\n",
    "        # Flatten tensors\n",
    "        y_pred_flat = y_pred.view(-1)\n",
    "        y_true_flat = y_true.view(-1)\n",
    "        \n",
    "        # Dice loss\n",
    "        intersection = (y_pred_flat * y_true_flat).sum()\n",
    "        dice = (2. * intersection + smooth) / (y_pred_flat.sum() + y_true_flat.sum() + smooth)\n",
    "        dice_loss = 1 - dice\n",
    "        \n",
    "        # BCE loss\n",
    "        bce_loss = self.bce(y_pred_flat, y_true_flat)\n",
    "        \n",
    "        return self.dice_weight * dice_loss + self.bce_weight * bce_loss\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    print(f\"üöÄ Starting training for {num_epochs} epochs\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    train_losses, val_losses, dice_scores = [], [], []\n",
    "    best_dice = 0.0\n",
    "    metrics_calc = SegmentationMetrics()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        \n",
    "        for imgs, masks in train_pbar:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            train_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_predictions = []\n",
    "        all_ground_truths = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                all_predictions.extend(outputs.cpu().numpy())\n",
    "                all_ground_truths.extend(masks.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = metrics_calc.evaluate_batch(all_ground_truths, all_predictions)\n",
    "        current_dice = metrics['dice_mean']\n",
    "        dice_scores.append(current_dice)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val Dice: {current_dice:.4f}\")\n",
    "        print(f\"  Val IoU: {metrics['iou_mean']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if current_dice > best_dice:\n",
    "            best_dice = current_dice\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"  ‚úÖ New best model saved! (Dice: {best_dice:.4f})\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return train_losses, val_losses, dice_scores\n",
    "\n",
    "def visualize_results(model, val_loader, n_samples=6):\n",
    "    model.eval()\n",
    "    imgs, masks = next(iter(val_loader))\n",
    "    with torch.no_grad():\n",
    "        imgs_device = imgs.to(device)\n",
    "        preds = model(imgs_device).cpu()\n",
    "    \n",
    "    fig, axes = plt.subplots(3, n_samples, figsize=(3*n_samples, 9))\n",
    "    for i in range(min(n_samples, len(imgs))):\n",
    "        img = imgs[i][0].numpy()\n",
    "        mask = masks[i].squeeze().numpy()\n",
    "        pred = preds[i].squeeze().numpy()\n",
    "        \n",
    "        axes[0, i].imshow(img, cmap='gray')\n",
    "        axes[0, i].set_title(f'Image {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(mask, cmap='gray')\n",
    "        axes[1, i].set_title('Ground Truth')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        axes[2, i].imshow(pred, cmap='gray')\n",
    "        axes[2, i].set_title(f'Prediction\\nDice: {SegmentationMetrics.dice_coefficient(mask, pred > 0.5):.3f}')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, dice_scores):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "    ax1.plot(val_losses, label='Val Loss', color='red')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training & Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(dice_scores, label='Dice Score', color='green')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Dice Score')\n",
    "    ax2.set_title('Validation Dice Score')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load dataset\n",
    "        print(\"üìÇ Loading dataset...\")\n",
    "        raw_files, mask_files = collect_file_pairs(raw_path, mask_path)\n",
    "        \n",
    "        if len(raw_files) == 0:\n",
    "            print(\"‚ùå No data found! Please check your dataset paths.\")\n",
    "            exit()\n",
    "        \n",
    "        # Split data\n",
    "        train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "            raw_files, mask_files, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create datasets and loaders\n",
    "        train_dataset = FemurDataset(train_images, train_masks)\n",
    "        val_dataset = FemurDataset(val_images, val_masks)\n",
    "        \n",
    "        batch_size = 4 if torch.cuda.is_available() else 2\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        \n",
    "        print(f\"üìä Training samples: {len(train_dataset)}\")\n",
    "        print(f\"üìä Validation samples: {len(val_dataset)}\")\n",
    "        \n",
    "        # Initialize model, loss, and optimizer\n",
    "        model = TransUNet(vit_pretrained=True).to(device)\n",
    "        criterion = DiceBCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "        \n",
    "        # Train model\n",
    "        train_losses, val_losses, dice_scores = train_model(\n",
    "            model, train_loader, val_loader, criterion, optimizer, num_epochs=5\n",
    "        )\n",
    "        \n",
    "        # Load best model and evaluate\n",
    "        model.load_state_dict(torch.load('best_model.pth'))\n",
    "        \n",
    "        # Final evaluation\n",
    "        print(\"\\nüî¨ Final Evaluation:\")\n",
    "        model.eval()\n",
    "        all_preds, all_truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                outputs = model(imgs.to(device))\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_truths.extend(masks.numpy())\n",
    "        \n",
    "        final_metrics = SegmentationMetrics.evaluate_batch(all_truths, all_preds)\n",
    "        print(f\"Final Dice Score: {final_metrics['dice_mean']:.4f} ¬± {final_metrics['dice_std']:.4f}\")\n",
    "        print(f\"Final IoU Score: {final_metrics['iou_mean']:.4f} ¬± {final_metrics['iou_std']:.4f}\")\n",
    "        \n",
    "        # Visualize results\n",
    "        plot_training_curves(train_losses, val_losses, dice_scores)\n",
    "        visualize_results(model, val_loader)\n",
    "        \n",
    "        print(\"\\nüéâ Training completed successfully!\")\n",
    "        print(f\"Best model saved as 'best_model.pth'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7850110,
     "sourceId": 12444573,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.226344,
   "end_time": "2025-07-21T20:20:35.939168",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-21T20:20:04.712824",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
